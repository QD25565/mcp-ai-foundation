#!/usr/bin/env python3
"""
NOTEBOOK MCP v6.1.0 - Small fixes and improved context exposure
===============================================
v6.0.0: DuckDB migration, PageRank <1s, native arrays
v6.1.0: Fixed timestamp bugs, removed all edge noise, preserved context

Changes in v6.1.0:
- Fixed empty pipe bug in timestamps
- Cleaner time format: YYYYMMDD|HHMM initially, then just HHMM for today
- Backend metrics only on explicit verbose=True

Built by AIs, for AIs.
===============================================
"""

import json
import sys
import os
import shutil
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
import logging
import random
import re
import time
import numpy as np
from cryptography.fernet import Fernet

# Database Engine
try:
    import duckdb
except ImportError:
    print("FATAL: DuckDB not installed. Please run 'pip install duckdb'", file=sys.stderr)
    sys.exit(1)

# Vector DB and embeddings
try:
    import chromadb
    from chromadb.config import Settings
    CHROMADB_AVAILABLE = True
except ImportError:
    CHROMADB_AVAILABLE = False
    logging.warning("ChromaDB not installed - semantic features disabled")

try:
    from sentence_transformers import SentenceTransformer
    ST_AVAILABLE = True
except ImportError:
    ST_AVAILABLE = False
    logging.warning("sentence-transformers not installed - semantic features disabled")

# Version
VERSION = "6.1.0"

# Configuration
OUTPUT_FORMAT = os.environ.get('NOTEBOOK_FORMAT', 'pipe')
USE_SEMANTIC = os.environ.get('NOTEBOOK_SEMANTIC', 'true').lower() == 'true'

# Limits
MAX_CONTENT_LENGTH = 5000
MAX_SUMMARY_LENGTH = 200
MAX_RESULTS = 100
BATCH_MAX = 50
DEFAULT_RECENT = 30
TEMPORAL_EDGES = 3
SESSION_GAP_MINUTES = 30
PAGERANK_ITERATIONS = 20
PAGERANK_DAMPING = 0.85
PAGERANK_CACHE_SECONDS = 300

# Storage paths
DATA_DIR = Path.home() / "AppData" / "Roaming" / "Claude" / "tools" / "notebook_data"
if not os.access(Path.home() / "AppData" / "Roaming", os.W_OK):
    DATA_DIR = Path(os.environ.get('TEMP', '/tmp')) / "notebook_data"

DATA_DIR.mkdir(parents=True, exist_ok=True)
DB_FILE = DATA_DIR / "notebook.duckdb"
SQLITE_DB_FILE = DATA_DIR / "notebook.db"
VECTOR_DIR = DATA_DIR / "vectors"
VAULT_KEY_FILE = DATA_DIR / ".vault_key"
LAST_OP_FILE = DATA_DIR / ".last_operation"

# Logging
logging.basicConfig(level=logging.INFO, stream=sys.stderr)

# Global State
KNOWN_ENTITIES = set()
KNOWN_TOOLS = {'teambook', 'firebase', 'gemini', 'claude', 'jetbrains', 'github',
                'slack', 'discord', 'vscode', 'git', 'docker', 'python', 'node',
                'react', 'vue', 'angular', 'tensorflow', 'pytorch', 'aws', 'gcp',
                'azure', 'kubernetes', 'redis', 'postgres', 'mongodb', 'sqlite',
                'task_manager', 'notebook', 'world', 'chromadb', 'duckdb'}
ENTITY_PATTERN = None
ENTITY_PATTERN_SIZE = 0
PAGERANK_DIRTY = True
PAGERANK_CACHE_TIME = 0
LAST_OPERATION = None
encoder = None
chroma_client = None
collection = None
EMBEDDING_MODEL = None
FTS_ENABLED = False

def get_persistent_id():
    """Get or create persistent AI identity"""
    for loc in [Path(__file__).parent, DATA_DIR, Path.home()]:
        id_file = loc / "ai_identity.txt"
        if id_file.exists():
            try:
                with open(id_file, 'r') as f:
                    stored_id = f.read().strip()
                    if stored_id: return stored_id
            except: pass
    
    adjectives = ['Swift', 'Bright', 'Sharp', 'Quick', 'Clear', 'Deep', 'Keen', 'Pure']
    nouns = ['Mind', 'Spark', 'Flow', 'Core', 'Sync', 'Node', 'Wave', 'Link']
    new_id = f"{random.choice(adjectives)}-{random.choice(nouns)}-{random.randint(100, 999)}"
    
    try:
        id_file = Path(__file__).parent / "ai_identity.txt"
        with open(id_file, 'w') as f: f.write(new_id)
    except: pass
    
    return new_id

CURRENT_AI_ID = os.environ.get('AI_ID', get_persistent_id())

class VaultManager:
    """Secure encrypted storage for secrets"""
    def __init__(self):
        self.key = self._load_or_create_key()
        self.fernet = Fernet(self.key)
    
    def _load_or_create_key(self) -> bytes:
        if VAULT_KEY_FILE.exists():
            with open(VAULT_KEY_FILE, 'rb') as f: return f.read()
        key = Fernet.generate_key()
        with open(VAULT_KEY_FILE, 'wb') as f: f.write(key)
        try:
            import stat
            os.chmod(VAULT_KEY_FILE, stat.S_IRUSR | stat.S_IWUSR)
        except: pass
        return key
    
    def encrypt(self, value: str) -> bytes:
        return self.fernet.encrypt(value.encode())
    
    def decrypt(self, encrypted: bytes) -> str:
        return self.fernet.decrypt(encrypted).decode()

vault_manager = VaultManager()

def get_db_conn() -> duckdb.DuckDBPyConnection:
    """Returns a new connection to the DuckDB database"""
    return duckdb.connect(database=str(DB_FILE), read_only=False)

def migrate_from_sqlite():
    """Migrate from SQLite to DuckDB if needed"""
    if DB_FILE.exists() or not SQLITE_DB_FILE.exists():
        return False
    
    logging.info("Migrating from SQLite to DuckDB...")
    
    try:
        import sqlite3
        sqlite_conn = sqlite3.connect(str(SQLITE_DB_FILE))
        sqlite_conn.row_factory = sqlite3.Row
        
        with get_db_conn() as duck_conn:
            _create_duckdb_schema(duck_conn)
            
            note_count = sqlite_conn.execute("SELECT COUNT(*) FROM notes").fetchone()[0]
            logging.info(f"Migrating {note_count} notes...")
            
            duck_conn.execute("BEGIN TRANSACTION")
            
            try:
                # Migrate notes with tags
                notes = sqlite_conn.execute("SELECT * FROM notes").fetchall()
                for note in notes:
                    # Get tags for this note
                    try:
                        tags = sqlite_conn.execute('''
                            SELECT t.name FROM tags t 
                            JOIN note_tags nt ON t.id = nt.tag_id 
                            WHERE nt.note_id = ?
                        ''', (note['id'],)).fetchall()
                        tag_list = [t['name'] for t in tags] if tags else []
                    except sqlite3.OperationalError:
                        tag_list = []
                        if 'tags' in note.keys():
                            tags_data = note['tags']
                            if tags_data:
                                try:
                                    tag_list = json.loads(tags_data) if isinstance(tags_data, str) else []
                                except: pass
                    
                    duck_conn.execute('''
                        INSERT INTO notes (
                            id, content, summary, tags, pinned, author,
                            created, session_id, linked_items, pagerank, has_vector
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        note['id'], note['content'], note['summary'], tag_list,
                        bool(note['pinned']), note['author'], note['created'],
                        note['session_id'], note['linked_items'],
                        note['pagerank'], bool(note['has_vector'])
                    ))
                
                # Migrate edges
                edges = sqlite_conn.execute("SELECT * FROM edges").fetchall()
                for edge in edges:
                    duck_conn.execute('''
                        INSERT INTO edges (from_id, to_id, type, weight, created)
                        VALUES (?, ?, ?, ?, ?)
                    ''', (edge['from_id'], edge['to_id'], edge['type'],
                          edge['weight'], edge['created']))
                
                # Migrate other tables
                _migrate_simple_table(sqlite_conn, duck_conn, 'entities')
                _migrate_simple_table(sqlite_conn, duck_conn, 'entity_notes')
                _migrate_simple_table(sqlite_conn, duck_conn, 'sessions')
                _migrate_simple_table(sqlite_conn, duck_conn, 'vault')
                _migrate_simple_table(sqlite_conn, duck_conn, 'stats')
                
                duck_conn.execute("COMMIT")
                logging.info("Migration committed successfully!")
                
                max_id = duck_conn.execute("SELECT MAX(id) FROM notes").fetchone()[0]
                if max_id:
                    duck_conn.execute(f"ALTER SEQUENCE notes_id_seq RESTART WITH {max_id + 1}")
                    logging.info(f"Sequence reset to start at {max_id + 1}")
                
            except Exception as e:
                duck_conn.execute("ROLLBACK")
                logging.error(f"Migration failed, rolled back: {e}")
                raise
        
        sqlite_conn.close()
        
        backup_path = SQLITE_DB_FILE.with_suffix(
            f'.backup_{datetime.now().strftime("%Y%m%d%H%M")}.db'
        )
        shutil.move(SQLITE_DB_FILE, backup_path)
        logging.info(f"Old database backed up to {backup_path}")
        
        return True
        
    except Exception as e:
        logging.error(f"Migration failed: {e}", exc_info=True)
        if DB_FILE.exists():
            os.remove(DB_FILE)
        sys.exit(1)

def _migrate_simple_table(sqlite_conn, duck_conn, table_name: str):
    """Helper to migrate a simple table"""
    try:
        rows = sqlite_conn.execute(f"SELECT * FROM {table_name}").fetchall()
        if rows:
            placeholders = ','.join(['?'] * len(rows[0]))
            duck_conn.executemany(
                f"INSERT INTO {table_name} VALUES ({placeholders})",
                rows
            )
    except Exception as e:
        logging.warning(f"Could not migrate {table_name}: {e}")

def _create_duckdb_schema(conn: duckdb.DuckDBPyConnection):
    """Create all tables and indices for DuckDB"""
    
    conn.execute("CREATE SEQUENCE IF NOT EXISTS notes_id_seq START 1")
    
    # Main notes table with native array for tags
    conn.execute('''
        CREATE TABLE IF NOT EXISTS notes (
            id BIGINT PRIMARY KEY,
            content TEXT,
            summary TEXT,
            tags VARCHAR[],
            pinned BOOLEAN DEFAULT FALSE,
            author VARCHAR NOT NULL,
            created TIMESTAMPTZ NOT NULL,
            session_id BIGINT,
            linked_items TEXT,
            pagerank DOUBLE DEFAULT 0.0,
            has_vector BOOLEAN DEFAULT FALSE
        )
    ''')
    
    # Edges table
    conn.execute('''
        CREATE TABLE IF NOT EXISTS edges (
            from_id BIGINT NOT NULL,
            to_id BIGINT NOT NULL,
            type VARCHAR NOT NULL,
            weight DOUBLE DEFAULT 1.0,
            created TIMESTAMPTZ NOT NULL,
            PRIMARY KEY(from_id, to_id, type)
        )
    ''')
    
    # Other tables
    conn.execute('''
        CREATE TABLE IF NOT EXISTS entities (
            id BIGINT PRIMARY KEY,
            name VARCHAR UNIQUE NOT NULL,
            type VARCHAR NOT NULL,
            first_seen TIMESTAMPTZ NOT NULL,
            last_seen TIMESTAMPTZ NOT NULL,
            mention_count INTEGER DEFAULT 1
        )
    ''')
    
    conn.execute('''
        CREATE TABLE IF NOT EXISTS entity_notes (
            entity_id BIGINT NOT NULL,
            note_id BIGINT NOT NULL,
            PRIMARY KEY(entity_id, note_id)
        )
    ''')
    
    conn.execute('''
        CREATE TABLE IF NOT EXISTS sessions (
            id BIGINT PRIMARY KEY,
            started TIMESTAMPTZ NOT NULL,
            ended TIMESTAMPTZ NOT NULL,
            note_count INTEGER DEFAULT 1,
            coherence_score DOUBLE DEFAULT 1.0
        )
    ''')
    
    conn.execute('''
        CREATE TABLE IF NOT EXISTS vault (
            key VARCHAR PRIMARY KEY,
            encrypted_value BLOB NOT NULL,
            created TIMESTAMPTZ NOT NULL,
            updated TIMESTAMPTZ NOT NULL,
            author VARCHAR NOT NULL
        )
    ''')
    
    conn.execute('''
        CREATE TABLE IF NOT EXISTS stats (
            id BIGINT PRIMARY KEY,
            operation VARCHAR NOT NULL,
            ts TIMESTAMPTZ NOT NULL,
            dur_ms INTEGER,
            author VARCHAR
        )
    ''')
    
    # Create indices
    conn.execute("CREATE INDEX IF NOT EXISTS idx_notes_created ON notes(created DESC)")
    conn.execute("CREATE INDEX IF NOT EXISTS idx_notes_pinned ON notes(pinned DESC, created DESC)")
    conn.execute("CREATE INDEX IF NOT EXISTS idx_notes_pagerank ON notes(pagerank DESC)")
    conn.execute("CREATE INDEX IF NOT EXISTS idx_edges_from ON edges(from_id)")
    conn.execute("CREATE INDEX IF NOT EXISTS idx_edges_to ON edges(to_id)")
    
    # Try to set up FTS
    global FTS_ENABLED
    try:
        conn.execute("INSTALL fts")
        conn.execute("LOAD fts")
        conn.execute("PRAGMA create_fts_index('notes', 'id', 'content', 'summary')")
        FTS_ENABLED = True
        logging.info("DuckDB FTS extension loaded")
    except Exception as e:
        FTS_ENABLED = False
        logging.warning(f"FTS not available: {e}")

def init_db():
    """Initialize DuckDB database"""
    migrate_from_sqlite()
    
    with get_db_conn() as conn:
        tables = conn.execute("SHOW TABLES").fetchall()
        if not any(t[0] == 'notes' for t in tables):
            logging.info("Creating new database schema...")
            _create_duckdb_schema(conn)
        else:
            try:
                max_id = conn.execute("SELECT MAX(id) FROM notes").fetchone()[0]
                if max_id:
                    seq_val = conn.execute("SELECT nextval('notes_id_seq')").fetchone()[0]
                    conn.execute(f"SELECT setval('notes_id_seq', {seq_val - 1})")
                    
                    if seq_val <= max_id:
                        conn.execute(f"ALTER SEQUENCE notes_id_seq RESTART WITH {max_id + 1}")
                        logging.info(f"Fixed sequence to start at {max_id + 1}")
            except Exception as e:
                logging.warning(f"Could not check/fix sequence: {e}")
        
        load_known_entities(conn)
        
        note_count = conn.execute("SELECT COUNT(*) FROM notes").fetchone()[0]
        logging.info(f"Database ready with {note_count} notes")

def load_known_entities(conn: duckdb.DuckDBPyConnection):
    """Load known entities into memory cache"""
    global KNOWN_ENTITIES
    try:
        entities = conn.execute('SELECT name FROM entities').fetchall()
        KNOWN_ENTITIES = {e[0].lower() for e in entities}
    except:
        KNOWN_ENTITIES = set()

def init_embedding_model():
    """Initialize embedding model"""
    global encoder, EMBEDDING_MODEL
    
    if not ST_AVAILABLE or not USE_SEMANTIC:
        logging.info("Semantic search disabled")
        return None
    
    try:
        models = [
            ('sentence-transformers/all-MiniLM-L6-v2', 'minilm'),
            ('BAAI/bge-base-en-v1.5', 'bge-base'),
        ]
        
        for model_name, short_name in models:
            try:
                logging.info(f"Loading {model_name}...")
                encoder = SentenceTransformer(model_name, device='cpu')
                test = encoder.encode("test", convert_to_numpy=True)
                EMBEDDING_MODEL = short_name
                logging.info(f"âœ“ Using {short_name} (dim: {test.shape[0]})")
                return encoder
            except Exception as e:
                logging.debug(f"Failed to load {model_name}: {e}")
        
        logging.error("No embedding model could be loaded")
        return None
    except Exception as e:
        logging.error(f"Failed to initialize embeddings: {e}")
        return None

def init_vector_db():
    """Initialize ChromaDB for vector storage"""
    global chroma_client, collection
    if not CHROMADB_AVAILABLE or not encoder:
        return False
    try:
        chroma_client = chromadb.PersistentClient(
            path=str(VECTOR_DIR),
            settings=Settings(anonymized_telemetry=False, allow_reset=True)
        )
        collection = chroma_client.get_or_create_collection(
            name="notebook_v6_duckdb",
            metadata={"hnsw:space": "cosine"}
        )
        logging.info(f"ChromaDB initialized with {collection.count()} vectors")
        return True
    except Exception as e:
        logging.error(f"ChromaDB init failed: {e}")
        return False

def save_last_operation(op_type: str, result: Any):
    """Save last operation for chaining"""
    global LAST_OPERATION
    LAST_OPERATION = {'type': op_type, 'result': result, 'time': datetime.now()}
    try:
        with open(LAST_OP_FILE, 'w') as f:
            json.dump({'type': op_type, 'time': LAST_OPERATION['time'].isoformat()}, f)
    except: pass

def get_last_operation() -> Optional[Dict]:
    """Get last operation for context"""
    global LAST_OPERATION
    if LAST_OPERATION:
        return LAST_OPERATION
    try:
        if LAST_OP_FILE.exists():
            with open(LAST_OP_FILE, 'r') as f:
                data = json.load(f)
                return {'type': data['type'], 'time': datetime.fromisoformat(data['time'])}
    except:
        pass
    return None

def pipe_escape(text: str) -> str:
    """Escape pipes in text for pipe format"""
    return str(text).replace('|', '\\|')

def format_time_compact(ts: Any) -> str:
    """Compact time format - YYYYMMDD|HHMM or just HHMM for today"""
    if not ts: 
        return "unknown"
    try:
        # Handle if ts is already a datetime object
        if isinstance(ts, datetime):
            dt = ts
        elif isinstance(ts, str):
            # Try parsing as ISO format
            dt = datetime.fromisoformat(ts.replace('Z', '+00:00').replace(' ', 'T'))
        else:
            # Try converting to string first
            dt = datetime.fromisoformat(str(ts))
        
        now = datetime.now()
        delta = now - dt
        
        if delta.total_seconds() < 60: 
            return "now"
        if delta.total_seconds() < 3600: 
            return f"{int(delta.total_seconds()/60)}m"
        if dt.date() == now.date():
            # Today - just show time
            return dt.strftime("%H%M")
        if delta.days == 1: 
            return f"y{dt.strftime('%H%M')}"
        if delta.days < 7: 
            return f"{delta.days}d"
        # Older - show full date|time
        return dt.strftime("%Y%m%d|%H%M")
    except Exception as e:
        # Log for debugging but return something useful
        logging.debug(f"Timestamp parsing error for {ts}: {e}")
        return str(ts)[:10] if ts else "unknown"

def clean_text(text: str) -> str:
    """Clean text by removing extra whitespace"""
    return re.sub(r'\s+', ' ', text).strip() if text else ""

def simple_summary(content: str, max_len: int = 150) -> str:
    """Create simple summary by truncating cleanly"""
    if not content: return ""
    clean = clean_text(content)
    if len(clean) <= max_len: return clean
    for sep in ['. ', '! ', '? ', '; ']:
        idx = clean.rfind(sep, 0, max_len)
        if idx > max_len * 0.5: return clean[:idx + 1]
    idx = clean.rfind(' ', 0, max_len - 3)
    if idx == -1 or idx < max_len * 0.7: idx = max_len - 3
    return clean[:idx] + "..."

def extract_references(content: str) -> List[int]:
    """Extract note references from content"""
    refs = set()
    for pattern in [r'note\s+(\d+)', r'\bn(\d+)\b', r'#(\d+)\b', r'\[(\d+)\]']:
        refs.update(int(m) for m in re.findall(pattern, content, re.IGNORECASE) if m.isdigit())
    return list(refs)

def extract_entities(content: str) -> List[Tuple[str, str]]:
    """Extract entities from content"""
    global ENTITY_PATTERN, ENTITY_PATTERN_SIZE
    entities = []
    content_lower = content.lower()
    
    mentions = re.findall(r'@([\w-]+)', content, re.IGNORECASE)
    entities.extend((m.lower(), 'mention') for m in mentions)
    
    all_known = KNOWN_TOOLS.union(KNOWN_ENTITIES)
    if all_known:
        if ENTITY_PATTERN is None or len(all_known) != ENTITY_PATTERN_SIZE:
            pattern_str = r'\b(' + '|'.join(re.escape(e) for e in all_known) + r')\b'
            ENTITY_PATTERN = re.compile(pattern_str, re.IGNORECASE)
            ENTITY_PATTERN_SIZE = len(all_known)
        
        if ENTITY_PATTERN:
            for entity_name in set(ENTITY_PATTERN.findall(content_lower)):
                entity_type = 'tool' if entity_name in KNOWN_TOOLS else 'known'
                entities.append((entity_name, entity_type))
    
    return entities

def detect_or_create_session(note_id: int, created: datetime, conn: duckdb.DuckDBPyConnection) -> Optional[int]:
    """Detect existing session or create new one"""
    try:
        prev = conn.execute(
            'SELECT created, session_id FROM notes WHERE id < ? ORDER BY id DESC LIMIT 1',
            [note_id]
        ).fetchone()
        
        if prev:
            prev_time = prev[0] if isinstance(prev[0], datetime) else datetime.fromisoformat(prev[0])
            if (created - prev_time).total_seconds() / 60 <= SESSION_GAP_MINUTES and prev[1]:
                conn.execute(
                    'UPDATE sessions SET ended = ?, note_count = note_count + 1 WHERE id = ?',
                    [created, prev[1]]
                )
                return prev[1]
        
        max_session_id = conn.execute("SELECT COALESCE(MAX(id), 0) FROM sessions").fetchone()[0]
        new_session_id = max_session_id + 1
        
        conn.execute(
            'INSERT INTO sessions (id, started, ended) VALUES (?, ?, ?)',
            [new_session_id, created, created]
        )
        return new_session_id
    except:
        return None

def create_all_edges(note_id: int, content: str, session_id: Optional[int], conn: duckdb.DuckDBPyConnection):
    """Create all edge types efficiently - backend only, never shown"""
    now = datetime.now()
    edges_to_add = []
    
    # Temporal edges
    prev_notes = conn.execute(
        'SELECT id FROM notes WHERE id < ? ORDER BY id DESC LIMIT ?',
        [note_id, TEMPORAL_EDGES]
    ).fetchall()
    for prev in prev_notes:
        edges_to_add.extend([
            (note_id, prev[0], 'temporal', 1.0, now),
            (prev[0], note_id, 'temporal', 1.0, now)
        ])
    
    # Reference edges
    refs = extract_references(content)
    if refs:
        placeholders = ','.join(['?'] * len(refs))
        valid_refs = conn.execute(
            f'SELECT id FROM notes WHERE id IN ({placeholders})',
            refs
        ).fetchall()
        for ref_id in valid_refs:
            edges_to_add.extend([
                (note_id, ref_id[0], 'reference', 2.0, now),
                (ref_id[0], note_id, 'referenced_by', 2.0, now)
            ])
    
    # Session edges
    if session_id:
        session_notes = conn.execute(
            'SELECT id FROM notes WHERE session_id = ? AND id != ?',
            [session_id, note_id]
        ).fetchall()
        for other in session_notes:
            edges_to_add.extend([
                (note_id, other[0], 'session', 1.5, now),
                (other[0], note_id, 'session', 1.5, now)
            ])
    
    # Entity edges
    entities = extract_entities(content)
    for entity_name, entity_type in entities:
        entity = conn.execute(
            'SELECT id FROM entities WHERE name = ?',
            [entity_name]
        ).fetchone()
        
        if entity:
            entity_id = entity[0]
            conn.execute(
                'UPDATE entities SET last_seen = ?, mention_count = mention_count + 1 WHERE id = ?',
                [now, entity_id]
            )
        else:
            max_entity_id = conn.execute("SELECT COALESCE(MAX(id), 0) FROM entities").fetchone()[0]
            entity_id = max_entity_id + 1
            
            conn.execute(
                'INSERT INTO entities (id, name, type, first_seen, last_seen) VALUES (?, ?, ?, ?, ?)',
                [entity_id, entity_name, entity_type, now, now]
            )
            if entity_id:
                KNOWN_ENTITIES.add(entity_name.lower())
        
        if entity_id:
            conn.execute(
                'INSERT INTO entity_notes (entity_id, note_id) VALUES (?, ?) ON CONFLICT DO NOTHING',
                [entity_id, note_id]
            )
            
            other_notes = conn.execute(
                'SELECT note_id FROM entity_notes WHERE entity_id = ? AND note_id != ?',
                [entity_id, note_id]
            ).fetchall()
            for other in other_notes:
                edges_to_add.extend([
                    (note_id, other[0], 'entity', 1.2, now),
                    (other[0], note_id, 'entity', 1.2, now)
                ])
    
    # Batch insert edges
    if edges_to_add:
        for edge in edges_to_add:
            conn.execute(
                'INSERT INTO edges (from_id, to_id, type, weight, created) VALUES (?, ?, ?, ?, ?) ON CONFLICT DO NOTHING',
                edge
            )

def calculate_pagerank_duckdb(conn: duckdb.DuckDBPyConnection):
    """Calculate PageRank using DuckDB's native SQL"""
    try:
        start = time.time()
        
        conn.execute(f'''
            CREATE OR REPLACE TEMPORARY TABLE pagerank_scores AS
            WITH RECURSIVE
            nodes AS (
                SELECT DISTINCT id FROM notes
            ),
            node_count AS (
                SELECT COUNT(*)::DOUBLE as total FROM nodes
            ),
            outlinks AS (
                SELECT from_id, SUM(weight) as total_weight
                FROM edges
                GROUP BY from_id
            ),
            pagerank(iteration, id, rank) AS (
                SELECT 0, id, 1.0 / node_count.total
                FROM nodes, node_count
                
                UNION ALL
                
                SELECT 
                    pr.iteration + 1,
                    n.id,
                    (1 - {PAGERANK_DAMPING}) / nc.total + 
                    {PAGERANK_DAMPING} * COALESCE(SUM(pr.rank * e.weight / ol.total_weight), 0)
                FROM nodes n
                CROSS JOIN node_count nc
                LEFT JOIN edges e ON e.to_id = n.id
                LEFT JOIN pagerank pr ON pr.id = e.from_id AND pr.iteration < {PAGERANK_ITERATIONS}
                LEFT JOIN outlinks ol ON ol.from_id = e.from_id
                WHERE pr.iteration < {PAGERANK_ITERATIONS}
                GROUP BY pr.iteration, n.id, nc.total
            )
            SELECT id, rank 
            FROM pagerank 
            WHERE iteration = {PAGERANK_ITERATIONS}
        ''')
        
        conn.execute('''
            UPDATE notes 
            SET pagerank = pr.rank
            FROM pagerank_scores pr
            WHERE notes.id = pr.id
        ''')
        
        elapsed = time.time() - start
        note_count = conn.execute("SELECT COUNT(*) FROM notes").fetchone()[0]
        logging.info(f"PageRank calculated for {note_count} notes in {elapsed:.2f}s")
        
    except Exception as e:
        logging.error(f"DuckDB PageRank failed, using fallback: {e}")
        conn.execute('''
            UPDATE notes 
            SET pagerank = COALESCE((
                SELECT COUNT(*) * 0.01 
                FROM edges 
                WHERE edges.to_id = notes.id
            ), 0.01)
        ''')

def calculate_pagerank_if_needed(conn: duckdb.DuckDBPyConnection):
    """Calculate PageRank when needed"""
    global PAGERANK_DIRTY, PAGERANK_CACHE_TIME
    
    count = conn.execute("SELECT COUNT(*) FROM notes").fetchone()[0]
    if count < 50:
        return
    
    current_time = time.time()
    if PAGERANK_DIRTY or (current_time - PAGERANK_CACHE_TIME > PAGERANK_CACHE_SECONDS):
        calculate_pagerank_duckdb(conn)
        PAGERANK_DIRTY = False
        PAGERANK_CACHE_TIME = current_time

def parse_time_query(when: str) -> Tuple[Optional[datetime], Optional[datetime]]:
    """Parse natural language time queries"""
    if not when:
        return None, None
    
    when_lower = when.lower().strip()
    now = datetime.now()
    today_start = now.replace(hour=0, minute=0, second=0, microsecond=0)
    
    if when_lower == "today":
        return today_start, now
    elif when_lower == "yesterday":
        yesterday_start = today_start - timedelta(days=1)
        return yesterday_start, today_start - timedelta(seconds=1)
    elif when_lower in ["this week", "week"]:
        week_start = today_start - timedelta(days=now.weekday())
        return week_start, now
    elif when_lower == "last week":
        week_start = today_start - timedelta(days=now.weekday())
        last_week_start = week_start - timedelta(days=7)
        return last_week_start, week_start
    
    return None, None

def log_operation(op: str, dur_ms: int = None):
    """Log operation for stats"""
    try:
        with get_db_conn() as conn:
            conn.execute(
                'INSERT INTO stats (operation, ts, dur_ms, author) VALUES (?, ?, ?, ?)',
                [op, datetime.now(), dur_ms, CURRENT_AI_ID]
            )
    except:
        pass

def _get_note_id(id_param: Any) -> Optional[int]:
    """Resolve 'last' or string IDs to integer"""
    if id_param == "last":
        last_op = get_last_operation()
        if last_op and last_op['type'] == 'remember':
            return last_op['result'].get('id')
        with get_db_conn() as conn:
            recent = conn.execute('SELECT id FROM notes ORDER BY created DESC LIMIT 1').fetchone()
            return recent[0] if recent else None
    
    if isinstance(id_param, str):
        clean_id = re.sub(r'[^\d]', '', id_param)
        return int(clean_id) if clean_id else None
    
    return int(id_param) if id_param is not None else None

def remember(content: str = None, summary: str = None, tags: List[str] = None, 
             linked_items: List[str] = None, **kwargs) -> Dict:
    """Save a note with DuckDB"""
    try:
        start = datetime.now()
        content = str(kwargs.get('content', content or '')).strip()
        if not content:
            content = f"Checkpoint {datetime.now().strftime('%H:%M')}"
        
        truncated = False
        orig_len = len(content)
        if orig_len > MAX_CONTENT_LENGTH:
            content = content[:MAX_CONTENT_LENGTH]
            truncated = True
        
        summary = clean_text(summary)[:MAX_SUMMARY_LENGTH] if summary else simple_summary(content)
        tags = [str(t).lower().strip() for t in tags if t] if tags else []
        
        with get_db_conn() as conn:
            max_id = conn.execute("SELECT COALESCE(MAX(id), 0) FROM notes").fetchone()[0]
            note_id = max_id + 1
            
            conn.execute('''
                INSERT INTO notes (
                    id, content, summary, tags, pinned, author,
                    created, session_id, linked_items, pagerank, has_vector
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', [
                note_id, content, summary, tags, False, CURRENT_AI_ID,
                datetime.now(), None,
                json.dumps(linked_items) if linked_items else None,
                0.0, bool(encoder and collection)
            ])
            
            session_id = detect_or_create_session(note_id, datetime.now(), conn)
            if session_id:
                conn.execute('UPDATE notes SET session_id = ? WHERE id = ?', [session_id, note_id])
            
            create_all_edges(note_id, content, session_id, conn)
            
            global PAGERANK_DIRTY
            PAGERANK_DIRTY = True
        
        # Add to vector store
        if encoder and collection:
            try:
                embedding = encoder.encode(content[:1000], convert_to_numpy=True)
                collection.add(
                    embeddings=[embedding.tolist()],
                    documents=[content],
                    metadatas={
                        "created": datetime.now().isoformat(),
                        "summary": summary,
                        "tags": json.dumps(tags)
                    },
                    ids=[str(note_id)]
                )
            except Exception as e:
                logging.warning(f"Vector storage failed: {e}")
        
        save_last_operation('remember', {'id': note_id, 'summary': summary})
        log_operation('remember', int((datetime.now() - start).total_seconds() * 1000))
        
        if OUTPUT_FORMAT == 'pipe':
            result_str = f"{note_id}|{format_time_compact(datetime.now())}|{summary}"
            if truncated:
                result_str += f"|T{orig_len}"
            return {"saved": result_str}
        else:
            result_dict = {"id": note_id, "time": format_time_compact(datetime.now()), "summary": summary}
            if truncated:
                result_dict["truncated"] = orig_len
            return result_dict
    
    except Exception as e:
        logging.error(f"Error in remember: {e}", exc_info=True)
        return {"error": f"Failed to save: {str(e)}"}

def recall(query: str = None, tag: str = None, when: str = None,
           pinned_only: bool = False, show_all: bool = False,
           limit: int = 50, mode: str = "hybrid", verbose: bool = False, **kwargs) -> Dict:
    """Search notes - always with rich summaries, all pinned shown"""
    try:
        start_time = datetime.now()
        
        if isinstance(limit, str):
            try:
                limit = int(limit)
            except:
                limit = 50
        
        if not any([show_all, query, tag, when, pinned_only]):
            limit = DEFAULT_RECENT
        
        with get_db_conn() as conn:
            calculate_pagerank_if_needed(conn)
            
            conditions = []
            params = []
            
            if pinned_only:
                conditions.append("pinned = TRUE")
            
            if when:
                time_start, time_end = parse_time_query(when)
                if time_start and time_end:
                    conditions.append("created BETWEEN ? AND ?")
                    params.extend([time_start, time_end])
            
            if tag:
                tag_clean = str(tag).lower().strip()
                conditions.append("list_contains(tags, ?)")
                params.append(tag_clean)
            
            notes = []
            
            # First, always get ALL pinned notes (unless specifically filtering)
            if not pinned_only and not query and not tag and not when:
                pinned_notes = conn.execute('''
                    SELECT id, content, summary, tags, pinned, author, created, pagerank
                    FROM notes WHERE pinned = TRUE
                    ORDER BY created DESC
                ''').fetchall()
            else:
                pinned_notes = []
            
            if query:
                # Semantic search
                semantic_ids = []
                if encoder and collection and mode in ["semantic", "hybrid"]:
                    try:
                        query_embedding = encoder.encode(str(query).strip(), convert_to_numpy=True)
                        results = collection.query(
                            query_embeddings=[query_embedding.tolist()],
                            n_results=min(limit, 100)
                        )
                        if results['ids'] and results['ids'][0]:
                            semantic_ids = [int(id_str) for id_str in results['ids'][0]]
                    except Exception as e:
                        logging.debug(f"Semantic search failed: {e}")
                
                # Keyword search
                keyword_ids = []
                if mode in ["keyword", "hybrid"]:
                    global FTS_ENABLED
                    if FTS_ENABLED:
                        try:
                            fts_results = conn.execute('''
                                SELECT fts_main_notes.id 
                                FROM fts_main_notes 
                                WHERE fts_main_notes MATCH ?
                                LIMIT ?
                            ''', [str(query).strip(), limit]).fetchall()
                            keyword_ids = [row[0] for row in fts_results]
                        except:
                            FTS_ENABLED = False
                    
                    if not FTS_ENABLED:
                        like_query = f"%{str(query).strip()}%"
                        like_results = conn.execute('''
                            SELECT id FROM notes 
                            WHERE content ILIKE ? OR summary ILIKE ?
                            ORDER BY pagerank DESC, created DESC
                            LIMIT ?
                        ''', [like_query, like_query, limit]).fetchall()
                        keyword_ids = [row[0] for row in like_results]
                
                # Combine results
                all_ids, seen = [], set()
                for i in range(max(len(semantic_ids), len(keyword_ids))):
                    if i < len(semantic_ids) and semantic_ids[i] not in seen:
                        all_ids.append(semantic_ids[i]); seen.add(semantic_ids[i])
                    if i < len(keyword_ids) and keyword_ids[i] not in seen:
                        all_ids.append(keyword_ids[i]); seen.add(keyword_ids[i])
                
                if all_ids:
                    note_ids = all_ids[:limit]
                    placeholders = ','.join(['?'] * len(note_ids))
                    
                    where_clause = " AND ".join(conditions) if conditions else "1=1"
                    final_params = note_ids + params + ([note_ids[0]] if note_ids else [])
                    
                    notes = conn.execute(f'''
                        SELECT id, content, summary, tags, pinned, author, created, pagerank
                        FROM notes
                        WHERE id IN ({placeholders}) AND {where_clause}
                        ORDER BY 
                            CASE WHEN id = ? THEN 0 ELSE 1 END,
                            pinned DESC, pagerank DESC, created DESC
                    ''', final_params).fetchall()
                else:
                    notes = []
            else:
                # Regular query without search
                where_clause = " WHERE " + " AND ".join(conditions) if conditions else ""
                notes = conn.execute(f'''
                    SELECT id, content, summary, tags, pinned, author, created, pagerank
                    FROM notes {where_clause}
                    ORDER BY pinned DESC, created DESC
                    LIMIT ?
                ''', params + [limit]).fetchall()
        
        # Combine pinned and regular notes
        all_notes = list(pinned_notes) + [n for n in notes if not n[4]]  # n[4] is pinned column
        
        if not all_notes:
            return {"msg": "No notes found"}
        
        if OUTPUT_FORMAT == 'pipe':
            lines = []
            for note in all_notes:
                note_id, content, summary, tags_arr, pinned, author, created, pagerank = note
                # Always preserve full summary - it's the main value
                parts = [
                    str(note_id),
                    format_time_compact(created),
                    summary or simple_summary(content, 150)  # Never truncate summaries
                ]
                if pinned:
                    parts.append('ðŸ“Œ')
                if verbose and pagerank and pagerank > 0.01:
                    parts.append(f"â˜…{pagerank:.2f}")
                lines.append('|'.join(pipe_escape(p) for p in parts))
            return {"notes": lines}
        else:
            formatted_notes = []
            for note in all_notes:
                note_id, content, summary, tags_arr, pinned, author, created, pagerank = note
                note_dict = {
                    'id': note_id,
                    'time': format_time_compact(created),
                    'summary': summary or simple_summary(content, 150),  # Never truncate
                }
                if pinned:
                    note_dict['pinned'] = True
                if verbose and pagerank and pagerank > 0.01:
                    note_dict['rank'] = round(pagerank, 3)
                formatted_notes.append(note_dict)
            return {"notes": formatted_notes}
        
        save_last_operation('recall', {"notes": all_notes})
        log_operation('recall', int((datetime.now() - start_time).total_seconds() * 1000))
        
    except Exception as e:
        logging.error(f"Error in recall: {e}", exc_info=True)
        return {"error": f"Recall failed: {str(e)}"}

def get_status(verbose: bool = False, **kwargs) -> Dict:
    """Get current state - ultra-minimal by default"""
    try:
        with get_db_conn() as conn:
            # Always get these essentials
            notes = conn.execute('SELECT COUNT(*) FROM notes').fetchone()[0]
            pinned = conn.execute('SELECT COUNT(*) FROM notes WHERE pinned = TRUE').fetchone()[0]
            
            recent = conn.execute('SELECT created FROM notes ORDER BY created DESC LIMIT 1').fetchone()
            last_activity = format_time_compact(recent[0]) if recent else "never"
            
            if verbose:
                # Backend metrics only when explicitly requested
                edges = conn.execute('SELECT COUNT(*) FROM edges').fetchone()[0]
                entities = conn.execute('SELECT COUNT(*) FROM entities').fetchone()[0]
                sessions = conn.execute('SELECT COUNT(*) FROM sessions').fetchone()[0]
                vault = conn.execute('SELECT COUNT(*) FROM vault').fetchone()[0]
                tags = conn.execute('SELECT COUNT(DISTINCT tag) FROM (SELECT unnest(tags) as tag FROM notes WHERE tags IS NOT NULL)').fetchone()[0]
                vector_count = collection.count() if collection else 0
                
                if OUTPUT_FORMAT == 'pipe':
                    parts = [
                        f"n:{notes}",
                        f"p:{pinned}",
                        f"v:{vector_count}",
                        f"e:{edges}",
                        f"ent:{entities}",
                        f"s:{sessions}",
                        f"t:{tags}",
                        f"last:{last_activity}",
                        f"db:duck",
                        f"emb:{EMBEDDING_MODEL or 'none'}"
                    ]
                    return {"status": '|'.join(parts)}
                else:
                    return {
                        "notes": notes,
                        "pinned": pinned,
                        "vectors": vector_count,
                        "edges": edges,
                        "entities": entities,
                        "sessions": sessions,
                        "tags": tags,
                        "last": last_activity,
                        "db": "duckdb",
                        "embedding": EMBEDDING_MODEL or "none"
                    }
            else:
                # Default minimal output - just what matters
                if OUTPUT_FORMAT == 'pipe':
                    return {"status": f"n:{notes}|p:{pinned}|{last_activity}"}
                else:
                    return {
                        "notes": notes,
                        "pinned": pinned,
                        "last": last_activity
                    }
    
    except Exception as e:
        logging.error(f"Error in get_status: {e}")
        return {"error": f"Status failed: {str(e)}"}

def _modify_pin_status(id_param: Any, pin: bool) -> Dict:
    """Helper to pin or unpin a note"""
    try:
        note_id = _get_note_id(id_param)
        if not note_id:
            return {"error": "Invalid note ID"}
        
        with get_db_conn() as conn:
            result = conn.execute(
                'UPDATE notes SET pinned = ? WHERE id = ? RETURNING summary, content',
                [pin, note_id]
            ).fetchone()
            
            if not result:
                return {"error": f"Note {note_id} not found"}
        
        action = 'pin' if pin else 'unpin'
        save_last_operation(action, {'id': note_id})
        
        if pin:
            summ = result[0] or simple_summary(result[1], 100)
            if OUTPUT_FORMAT == 'pipe':
                return {"pinned": f"{note_id}|{summ}"}
            else:
                return {"pinned": note_id, "summary": summ}
        else:
            return {"unpinned": note_id}
    
    except Exception as e:
        logging.error(f"Error in pin/unpin: {e}")
        return {"error": f"Failed to {action}: {str(e)}"}

def pin_note(id: Any = None, **kwargs) -> Dict:
    """Pin a note"""
    return _modify_pin_status(kwargs.get('id', id), True)

def unpin_note(id: Any = None, **kwargs) -> Dict:
    """Unpin a note"""
    return _modify_pin_status(kwargs.get('id', id), False)

def get_full_note(id: Any = None, verbose: bool = False, **kwargs) -> Dict:
    """Get complete note - NO edges ever shown"""
    try:
        note_id = _get_note_id(kwargs.get('id', id))
        if not note_id:
            return {"error": "Invalid note ID"}
        
        with get_db_conn() as conn:
            note = conn.execute('SELECT * FROM notes WHERE id = ?', [note_id]).fetchone()
            if not note:
                return {"error": f"Note {note_id} not found"}
            
            cols = [desc[0] for desc in conn.description]
            result = dict(zip(cols, note))
            
            # Clean up datetime
            if 'created' in result and result['created']:
                result['created'] = format_time_compact(result['created'])
            
            # Get entities (actually useful for pattern matching)
            entities = conn.execute('''
                SELECT e.name FROM entities e
                JOIN entity_notes en ON e.id = en.entity_id
                WHERE en.note_id = ?
            ''', [note_id]).fetchall()
            if entities:
                result['entities'] = [e[0] for e in entities]
            
            # Remove any edge data from result
            result.pop('session_id', None)
            result.pop('linked_items', None)
            result.pop('pagerank', None)
            result.pop('has_vector', None)
            
            # NEVER include edges - they're backend noise
        
        save_last_operation('get_full_note', {'id': note_id})
        return result
    
    except Exception as e:
        logging.error(f"Error in get_full_note: {e}", exc_info=True)
        return {"error": f"Failed to retrieve: {str(e)}"}

def vault_store(key: str = None, value: str = None, **kwargs) -> Dict:
    """Store encrypted secret"""
    try:
        key = str(kwargs.get('key', key) or '').strip()
        value = str(kwargs.get('value', value) or '').strip()
        if not key or not value:
            return {"error": "Key and value required"}
        
        encrypted = vault_manager.encrypt(value)
        now = datetime.now()
        
        with get_db_conn() as conn:
            conn.execute('''
                INSERT INTO vault (key, encrypted_value, created, updated, author)
                VALUES (?, ?, ?, ?, ?)
                ON CONFLICT (key) DO UPDATE SET
                    encrypted_value = EXCLUDED.encrypted_value,
                    updated = EXCLUDED.updated
            ''', [key, encrypted, now, now, CURRENT_AI_ID])
        
        log_operation('vault_store')
        return {"stored": key}
    
    except Exception as e:
        logging.error(f"Error in vault_store: {e}")
        return {"error": f"Storage failed: {str(e)}"}

def vault_retrieve(key: str = None, **kwargs) -> Dict:
    """Retrieve decrypted secret"""
    try:
        key = str(kwargs.get('key', key) or '').strip()
        if not key:
            return {"error": "Key required"}
        
        with get_db_conn() as conn:
            result = conn.execute(
                'SELECT encrypted_value FROM vault WHERE key = ?',
                [key]
            ).fetchone()
        
        if not result:
            return {"error": f"Key '{key}' not found"}
        
        decrypted = vault_manager.decrypt(result[0])
        log_operation('vault_retrieve')
        return {"key": key, "value": decrypted}
    
    except Exception as e:
        logging.error(f"Error in vault_retrieve: {e}")
        return {"error": f"Retrieval failed: {str(e)}"}

def vault_list(**kwargs) -> Dict:
    """List vault keys"""
    try:
        with get_db_conn() as conn:
            items = conn.execute(
                'SELECT key, updated FROM vault ORDER BY updated DESC'
            ).fetchall()
        
        if not items:
            return {"msg": "Vault empty"}
        
        if OUTPUT_FORMAT == 'pipe':
            keys = []
            for key, updated in items:
                keys.append(f"{key}|{format_time_compact(updated)}")
            return {"vault_keys": keys}
        else:
            keys = [
                {'key': key, 'updated': format_time_compact(updated)}
                for key, updated in items
            ]
            return {"vault_keys": keys}
    
    except Exception as e:
        logging.error(f"Error in vault_list: {e}")
        return {"error": f"List failed: {str(e)}"}

def batch(operations: List[Dict] = None, **kwargs) -> Dict:
    """Execute multiple operations efficiently"""
    try:
        operations = kwargs.get('operations', operations or [])
        if not operations:
            return {"error": "No operations"}
        if len(operations) > BATCH_MAX:
            return {"error": f"Max {BATCH_MAX} operations"}
        
        op_map = {
            'remember': remember, 'recall': recall,
            'pin_note': pin_note, 'pin': pin_note,
            'unpin_note': unpin_note, 'unpin': unpin_note,
            'vault_store': vault_store, 'vault_retrieve': vault_retrieve,
            'get_full_note': get_full_note, 'get': get_full_note,
            'status': get_status, 'vault_list': vault_list
        }
        
        results = []
        for op in operations:
            op_type = op.get('type')
            if op_type in op_map:
                results.append(op_map[op_type](**op.get('args', {})))
            else:
                results.append({"error": f"Unknown op: {op_type}"})
        
        return {"batch_results": results, "count": len(results)}
    
    except Exception as e:
        logging.error(f"Error in batch: {e}")
        return {"error": f"Batch failed: {str(e)}"}

def handle_tools_call(params: Dict) -> Dict:
    """Route tool calls with clean formatting"""
    tool_name = params.get("name", "").lower().strip()
    tool_args = params.get("arguments", {})
    
    tools = {
        "get_status": get_status, "remember": remember, "recall": recall,
        "get_full_note": get_full_note, "get": get_full_note,
        "pin_note": pin_note, "pin": pin_note,
        "unpin_note": unpin_note, "unpin": unpin_note,
        "vault_store": vault_store, "vault_retrieve": vault_retrieve,
        "vault_list": vault_list, "batch": batch
    }
    
    if tool_name not in tools:
        return {"content": [{"type": "text", "text": f"Error: Unknown tool: {tool_name}"}]}
    
    result = tools[tool_name](**tool_args)
    text_parts = []
    
    # Format response based on tool and result
    if tool_name in ["get_full_note", "get"] and "content" in result and "id" in result:
        text_parts.append(f"=== NOTE {result['id']} ===")
        if result.get('pinned'):
            text_parts.append("ðŸ“Œ PINNED")
        text_parts.append(f"\n{result['content']}\n")
        if result.get('summary'):
            text_parts.append(f"Summary: {result['summary']}")
        if result.get('entities'):
            text_parts.append(f"Entities: {', '.join(result['entities'])}")
        # NO edge data ever shown
    elif tool_name == "vault_retrieve" and "value" in result:
        text_parts.append(f"ðŸ” {result['key']}: {result['value']}")
    elif "error" in result:
        text_parts.append(f"Error: {result['error']}")
    elif OUTPUT_FORMAT == 'pipe' and "notes" in result and isinstance(result["notes"], list):
        text_parts.extend(result["notes"])
    elif "saved" in result:
        text_parts.append(result["saved"])
    elif "pinned" in result:
        text_parts.append(str(result["pinned"]))
    elif "unpinned" in result:
        text_parts.append(f"Unpinned {result['unpinned']}")
    elif "stored" in result:
        text_parts.append(f"Stored {result['stored']}")
    elif "status" in result:
        text_parts.append(result["status"])
    elif "vault_keys" in result:
        if OUTPUT_FORMAT == 'pipe':
            text_parts.extend(result["vault_keys"])
        else:
            text_parts.append(json.dumps(result["vault_keys"]))
    elif "msg" in result:
        text_parts.append(result["msg"])
    elif "batch_results" in result:
        text_parts.append(f"Batch: {result.get('count', 0)}")
        for r in result["batch_results"]:
            if isinstance(r, dict):
                if "error" in r:
                    text_parts.append(f"Error: {r['error']}")
                elif "saved" in r:
                    text_parts.append(r["saved"])
                elif "pinned" in r:
                    text_parts.append(str(r["pinned"]))
                else:
                    text_parts.append(json.dumps(r))
            else:
                text_parts.append(str(r))
    else:
        text_parts.append(json.dumps(result))
    
    return {"content": [{"type": "text", "text": "\n".join(text_parts) if text_parts else "Done"}]}

# Initialize everything on import
init_db()
init_embedding_model()
init_vector_db()

def main():
    """MCP server main loop"""
    logging.info(f"Notebook MCP v{VERSION} - AI-Only Edition")
    logging.info(f"Identity: {CURRENT_AI_ID} | DB: {DB_FILE}")
    logging.info(f"Features: Rich content preserved, backend noise removed")
    logging.info(f"Embedding: {EMBEDDING_MODEL or 'None'}")
    logging.info(f"FTS: {'Yes' if FTS_ENABLED else 'No'}")
    logging.info("âœ“ Fixed timestamp formatting")
    logging.info("âœ“ All pinned notes shown")
    logging.info("âœ“ No edge data pollution")
    
    while True:
        try:
            line = sys.stdin.readline()
            if not line:
                break
            line = line.strip()
            if not line:
                continue
            
            request = json.loads(line)
            request_id = request.get("id")
            method = request.get("method", "")
            params = request.get("params", {})
            
            response = {"jsonrpc": "2.0", "id": request_id}
            
            if method == "initialize":
                response["result"] = {
                    "protocolVersion": "2024-11-05",
                    "capabilities": {"tools": {}},
                    "serverInfo": {
                        "name": "notebook",
                        "version": VERSION,
                        "description": f"AI memory: Rich summaries, no noise, {EMBEDDING_MODEL or 'keyword'}"
                    }
                }
            elif method == "notifications/initialized":
                continue
            elif method == "tools/list":
                tool_schemas = {
                    "get_status": {
                        "desc": "System state (n:X|p:Y|last)",
                        "props": {"verbose": {"type": "boolean", "description": "Include backend metrics"}}
                    },
                    "remember": {
                        "desc": "Save a note",
                        "props": {
                            "content": {"type": "string"},
                            "summary": {"type": "string"},
                            "tags": {"type": "array", "items": {"type": "string"}}
                        }
                    },
                    "recall": {
                        "desc": "Search notes (shows ALL pinned + results)",
                        "props": {
                            "query": {"type": "string"},
                            "tag": {"type": "string"},
                            "when": {"type": "string"},
                            "verbose": {"type": "boolean", "description": "Include PageRank scores"}
                        }
                    },
                    "get_full_note": {
                        "desc": "Get note content",
                        "props": {
                            "id": {"type": "string"},
                            "verbose": {"type": "boolean", "description": "No effect - edges never shown"}
                        },
                        "req": ["id"]
                    },
                    "get": {
                        "desc": "Alias for get_full_note",
                        "props": {
                            "id": {"type": "string"},
                            "verbose": {"type": "boolean", "description": "No effect - edges never shown"}
                        },
                        "req": ["id"]
                    },
                    "pin_note": {
                        "desc": "Pin a note",
                        "props": {"id": {"type": "string"}},
                        "req": ["id"]
                    },
                    "pin": {
                        "desc": "Alias for pin_note",
                        "props": {"id": {"type": "string"}},
                        "req": ["id"]
                    },
                    "unpin_note": {
                        "desc": "Unpin a note",
                        "props": {"id": {"type": "string"}},
                        "req": ["id"]
                    },
                    "unpin": {
                        "desc": "Alias for unpin_note",
                        "props": {"id": {"type": "string"}},
                        "req": ["id"]
                    },
                    "vault_store": {
                        "desc": "Store encrypted secret",
                        "props": {"key": {"type": "string"}, "value": {"type": "string"}},
                        "req": ["key", "value"]
                    },
                    "vault_retrieve": {
                        "desc": "Retrieve decrypted secret",
                        "props": {"key": {"type": "string"}},
                        "req": ["key"]
                    },
                    "vault_list": {
                        "desc": "List vault keys",
                        "props": {}
                    },
                    "batch": {
                        "desc": "Execute multiple operations",
                        "props": {"operations": {"type": "array"}},
                        "req": ["operations"]
                    },
                }
                
                response["result"] = {
                    "tools": [{
                        "name": name,
                        "description": schema["desc"],
                        "inputSchema": {
                            "type": "object",
                            "properties": schema["props"],
                            "required": schema.get("req", []),
                            "additionalProperties": True
                        }
                    } for name, schema in tool_schemas.items()]
                }
            elif method == "tools/call":
                response["result"] = handle_tools_call(params)
            else:
                response["result"] = {"status": "ready"}
            
            if "result" in response or "error" in response:
                print(json.dumps(response), flush=True)
        
        except (KeyboardInterrupt, SystemExit):
            break
        except Exception as e:
            logging.error(f"Server loop error: {e}", exc_info=True)
            continue
    
    logging.info("Notebook MCP shutting down")

if __name__ == "__main__":
    main()
